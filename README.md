# Enterprise RAG System with AI Agent

An enterprise-grade Retrieval-Augmented Generation (RAG) system built with LangChain, featuring an intelligent AI agent that can autonomously route queries to the appropriate tools.

## ğŸŒŸ Features

### RAG Pipeline
- **Hybrid Search**: Combines dense (semantic) and sparse (keyword) retrieval
- **LLM Reranking**: Uses GPT-4o-mini to rerank retrieved chunks for better relevance
- **Rich Metadata**: Extracts and stores comprehensive metadata for filtering
- **Citations**: Provides exact quotes from source documents

### AI Agent (ReAct Pattern)
- **Intelligent Routing**: Automatically determines the best tool for each query
- **Multi-Tool Support**: RAG, API Query, and Ticket Escalation
- **Conversation Memory**: Maintains context across multiple turns
- **Transparency**: Shows which tools are being used and why

### Tools
| Tool | Purpose | Trigger |
|------|---------|---------|
| **RAG Knowledge Base** | Technical docs, concepts, best practices | "What is...", "How does...", "Explain..." |
| **Query API** | Agent status, token usage, billing, metrics | "How many tokens...", "Show my agents..." |
| **Ticket Escalation** | Create support tickets | "Talk to human", low confidence, errors |

## ğŸ“ Project Structure

```
halter-challenge-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                    # AI Agent system
â”‚   â”‚   â”œâ”€â”€ orchestrator.py        # Main agent with tool calling
â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”‚       â”œâ”€â”€ rag_tool.py        # RAG knowledge base tool
â”‚   â”‚       â”œâ”€â”€ query_api_tool.py  # API query tool
â”‚   â”‚       â””â”€â”€ ticket_tool.py     # Ticket escalation tool
â”‚   â”œâ”€â”€ retriever/                 # Retrieval components
â”‚   â”‚   â”œâ”€â”€ search.py              # Basic semantic search
â”‚   â”‚   â”œâ”€â”€ hybrid_search.py       # Hybrid search (dense + sparse)
â”‚   â”‚   â”œâ”€â”€ reranker.py            # LLM-based reranking
â”‚   â”‚   â””â”€â”€ pipeline.py            # Full retrieval pipeline
â”‚   â”œâ”€â”€ generator/                 # Generation components
â”‚   â”‚   â””â”€â”€ rag_generator.py       # Answer generation with citations
â”‚   â”œâ”€â”€ vectorstore/               # Vector database
â”‚   â”‚   â”œâ”€â”€ embeddings.py          # OpenAI embeddings
â”‚   â”‚   â””â”€â”€ pinecone_store.py      # Pinecone vector store
â”‚   â”œâ”€â”€ document_processing/       # Document processing
â”‚   â”‚   â”œâ”€â”€ chunker.py             # Markdown chunking
â”‚   â”‚   â””â”€â”€ metadata_extractor.py  # Rich metadata extraction
â”‚   â”œâ”€â”€ data/                      # Mock data
â”‚   â”‚   â””â”€â”€ mock_api_data.py       # Simulated API responses
â”‚   â””â”€â”€ prompts.py                 # Centralized prompt management
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ agent.py                   # Interactive AI agent
â”‚   â”œâ”€â”€ rag.py                     # Standalone RAG Q&A
â”‚   â”œâ”€â”€ chunking.py                # Document chunking
â”‚   â””â”€â”€ upload_to_pinecone.py      # Vector upload
â”œâ”€â”€ raw_data/                      # Source documents
â”œâ”€â”€ processed_data/                # Processed chunks (JSON)
â”œâ”€â”€ logs/                          # Support tickets
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                           # API keys (not committed)
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repo-url>
cd halter-challenge-rag

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

Create a `.env` file in the project root:

```bash
# OpenAI API Key
OPENAI_API_KEY=sk-your-openai-key

# Pinecone Configuration
PINECONE_API_KEY=your-pinecone-key
PINECONE_INDEX_NAME=your-index-name

# Embedding Model
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIMENSION=1024
```

### 3. Process Documents

```bash
# Chunk documents and extract metadata
python scripts/chunking.py

# Upload to Pinecone
python scripts/upload_to_pinecone.py
```

### 4. Run the AI Agent

```bash
python scripts/agent.py
```

## ğŸ’¡ Usage Examples

### AI Agent (Recommended)

```bash
python scripts/agent.py
```

```
â“ You: What is the difference between workflows and agents?
ğŸ”„ Processing...
ğŸ¤– Running agent to determine best tool...
ğŸ”§ Tool Selected: rag_knowledge_base

ğŸ“š Response [RAG]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The key difference lies in who controls the process:

**Workflows:** Systems with predefined code paths...
**Agents:** Systems where LLMs dynamically direct their own processes...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â“ You: How many tokens have I used?
ğŸ”„ Processing...
ğŸ”§ Tool Selected: query_api

ğŸ”Œ Response [API]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ˆ **Token Usage Summary**
**Total Tokens:** 4,200,000
**Quota:** 45.0% used
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Standalone RAG

```bash
python scripts/rag.py
```

### Programmatic Usage

```python
from src.agents import AgentOrchestrator

# Initialize agent
agent = AgentOrchestrator(
    model="gpt-4o-mini",
    memory_window=10,
    verbose=True
)

# Ask questions
response = agent.ask("What is an agent?")
print(response.answer)
print(f"Source: {response.source}")
print(f"Tools used: {response.tool_calls}")
```

## ğŸ”§ Architecture

### RAG Pipeline

```
Query â†’ Hybrid Search â†’ LLM Reranking â†’ Context Augmentation â†’ Generation
         â†“                  â†“                   â†“                   â†“
    Dense + Sparse    GPT-4o-mini         Format + Compress    Structured Output
    (top 15)          (top 5)             with metadata        with citations
```

### Agent Flow

```
User Question
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Agent Orchestrator            â”‚
â”‚      (Tool Calling Pattern)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG  â”‚   â”‚  API   â”‚   â”‚ Ticket  â”‚
â”‚ Tool  â”‚   â”‚ Query  â”‚   â”‚  Tool   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Metadata Schema

Each chunk includes rich metadata for filtering:

| Field | Type | Description |
|-------|------|-------------|
| `doc_title` | string | Document title |
| `section_hierarchy` | list | Section path (e.g., ["Building blocks", "Agents"]) |
| `content_type` | string | definition, example, workflow, appendix |
| `workflow_type` | string | routing, parallelization, agents, etc. |
| `has_diagram` | bool | Contains diagrams |
| `has_code` | bool | Contains code |
| `topics` | list | Extracted topics |

## ğŸ›ï¸ Configuration Options

### RAG Pipeline

```python
from src.retriever.pipeline import PipelineConfig

config = PipelineConfig(
    namespace="agents-doc",
    initial_k=15,           # Initial retrieval count
    final_k=5,              # After reranking
    use_hybrid=True,        # Enable hybrid search
    use_reranking=True,     # Enable LLM reranking
    vector_weight=0.3,      # Weight for vector score
    rerank_weight=0.7,      # Weight for LLM score
)
```

### Generator

```python
from src.generator import GeneratorConfig

config = GeneratorConfig(
    model="gpt-4o-mini",
    temperature=0.1,
    max_tokens=2000,
    use_structured_output=True,
)
```

## ğŸ“ Prompts

All prompts are centralized in `src/prompts.py` with modular structure:

- `RAGAnswerPrompt` - Knowledge base Q&A
- `RerankingPrompt` - LLM reranking
- `QueryExpansionPrompt` - Query expansion
- `ContextCompressionPrompt` - Context compression

## ğŸ”’ Ticket System

When the agent cannot answer or user requests human help:

```json
{
  "ticket_id": "TKT-20241212-001",
  "priority": "medium",
  "reason": "low_confidence",
  "user_question": "...",
  "status": "open",
  "created_at": "2024-12-12T10:30:00Z"
}
```

Tickets are saved to `logs/ticket_*.json`.

## ğŸ› ï¸ Development

### Adding New Tools

1. Create tool in `src/agents/tools/`
2. Inherit from `BaseTool`
3. Define `name`, `description`, `args_schema`
4. Implement `_run()` method
5. Add to `orchestrator.py` tools list

### Adding New Documents

1. Place markdown files in `raw_data/`
2. Run `python scripts/chunking.py`
3. Run `python scripts/upload_to_pinecone.py`

## ğŸ“¦ Dependencies

- **LangChain** - Agent framework
- **OpenAI** - LLM and embeddings
- **Pinecone** - Vector database
- **Pydantic** - Data validation
- **Loguru** - Logging

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- Built with [LangChain](https://langchain.com/)
- Vector storage by [Pinecone](https://pinecone.io/)
- Inspired by Anthropic's "Building Effective Agents" guide
